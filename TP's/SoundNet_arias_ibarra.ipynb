{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoundNet: Learning Sound\n",
    "Representations from Unlabeled Video\n",
    "---\n",
    "---\n",
    "<img src=\"../TP's/images/IMT_Atlantique.jpg\" width=\"200\">\n",
    "\n",
    "Authors:\n",
    "`ARIAS Camila and IBARRA Kevin`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is explain how `SoundNet` works (the maths, the code and the experiments). Soundnet  was developed in 2016 in order to use the natural synchronization between vision and sound to learn an acoustic representation from a large amount of unlabeled videos.\n",
    "\n",
    "[Scientific article reference](https://arxiv.org/pdf/1610.09001.pdf)  by Yusuf Aytar, Carl Vondrick, Antonio Torralba. NIPS 2016\n",
    "\n",
    "[SoundNet in Keras](https://github.com/brain-bzh/soundnet_keras) SoundNet, built in Keras with pre-trained 8-layer model.\n",
    "\n",
    "\n",
    "\n",
    "# Presentation of the problem"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Describir el problema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of works related to object recognition, speech recognition and machine translation using labeled dataset as reference but it is not the same in understanding of natural sound maybe is because is expensive and ambiguos to collect labeled sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">How to transfer discriminative visual knowlegde into sound using unlabeled video as a brigde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep convolutional network learns directly from **raw audio** waveform but the model is training with visual supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other studies are focus on features such as spectograms and MFC, on other hand,  Soundnet is focus in the natural sound. \n",
    "* 2M Train --deep fully convolutional network\n",
    "* Teacher-student model **Transfer model**\n",
    "* Without truth sound labels\n",
    "* Videos Flick\n",
    "* sound mp3 explicar en el codifgo\n",
    "\n",
    "DESCRIBIR DATASET           \n",
    "## Architecture\n",
    "![sound.jpeg](../TP's/images/sound.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoundNet is a deep convolutional network, layers are described as follow:\n",
    "    \n",
    "1. `One dimensional convolutions` \n",
    "\n",
    "    Why does it use convolutional into sound data?\n",
    "    Because they are invariables to translation making the number of parameters are reduced \n",
    "    \n",
    "   stack layers: Detect high level features **\n",
    "\n",
    "2. `Fully connected`\n",
    "3. `Pooling` \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math in training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are explain the math behind this project. It is important make a mention in the training phase they used video and sound, but the aim is learn by sound, so in the compile model there is not video as input. \n",
    "\n",
    "*Training phase*\n",
    "\n",
    "For the inputs the model needs the raw audio waveform $$x_i \\in \\mathcal{R}^D$$ and the video $$y_i \\in \\mathcal{R}^{3xTxWxH}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key aspects of the learning setting\n",
    "\n",
    "SOUND CLASSIFICATION\n",
    "IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code: model pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to understand the code and explain how it is works. \n",
    "\n",
    "The code given by the teacher staff correspond to [SoundNet](https://github.com/brain-bzh/soundnet_keras) built in Keras with pre-trained **8-layer model**. \n",
    "\n",
    "So, the first thing to do is obtain a model with the weigths pre-trained. \n",
    "Let's import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from keras.layers import BatchNormalization, Activation, Conv1D, MaxPooling1D, ZeroPadding1D, InputLayer\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import librosa \n",
    "#audio library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The configuration of the layers \n",
    "\n",
    "<img src=\"../TP's/images/layers.png\" width=\"600\">\n",
    "\n",
    "Now `build_model` make soundNet according to the structure defined by the authors. This model has as input the audio raw waveform and two output:scenes and objet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Builds up the SoundNet model and loads the weights from a given model file (8-layer model is kept at models/sound8.npy).\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model_weights = np.load('../soundnet_keras-master/models/sound8.npy',encoding = 'latin1').item()\n",
    "    model = Sequential()\n",
    "    #Input layer: audio raw waveform (1,length_audio,1)\n",
    "    model.add(InputLayer(batch_input_shape=(1, None, 1)))\n",
    "\n",
    "    filter_parameters = [{'name': 'conv1', 'num_filters': 16, 'padding': 32,\n",
    "                          'kernel_size': 64, 'conv_strides': 2,\n",
    "                          'pool_size': 8, 'pool_strides': 8}, #pool1\n",
    "\n",
    "                         {'name': 'conv2', 'num_filters': 32, 'padding': 16,\n",
    "                          'kernel_size': 32, 'conv_strides': 2,\n",
    "                          'pool_size': 8, 'pool_strides': 8}, #pool2\n",
    "\n",
    "                         {'name': 'conv3', 'num_filters': 64, 'padding': 8,\n",
    "                          'kernel_size': 16, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv4', 'num_filters': 128, 'padding': 4,\n",
    "                          'kernel_size': 8, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv5', 'num_filters': 256, 'padding': 2,\n",
    "                          'kernel_size': 4, 'conv_strides': 2,\n",
    "                          'pool_size': 4, 'pool_strides': 4}, #pool5\n",
    "\n",
    "                         {'name': 'conv6', 'num_filters': 512, 'padding': 2,\n",
    "                          'kernel_size': 4, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv7', 'num_filters': 1024, 'padding': 2,\n",
    "                          'kernel_size': 4, 'conv_strides': 2},\n",
    "\n",
    "                         {'name': 'conv8_2', 'num_filters': 401, 'padding': 0,\n",
    "                          'kernel_size': 8, 'conv_strides': 2},#output: VGG 401 classes\n",
    "                         ]\n",
    "\n",
    "    for x in filter_parameters:\n",
    "        #for each [zero_padding - conv - batchNormalization - relu]\n",
    "        model.add(ZeroPadding1D(padding=x['padding']))\n",
    "        model.add(Conv1D(x['num_filters'],\n",
    "                         kernel_size=x['kernel_size'],\n",
    "                         strides=x['conv_strides'],\n",
    "                         padding='valid'))\n",
    "        weights = model_weights[x['name']]['weights'].reshape(model.layers[-1].get_weights()[0].shape)\n",
    "        biases = model_weights[x['name']]['biases']\n",
    "\n",
    "        model.layers[-1].set_weights([weights, biases])  #set weights in conv\n",
    "\n",
    "        if 'conv8' not in x['name']:\n",
    "            gamma = model_weights[x['name']]['gamma']\n",
    "            beta = model_weights[x['name']]['beta']\n",
    "            mean = model_weights[x['name']]['mean']\n",
    "            var = model_weights[x['name']]['var']\n",
    "\n",
    "            \n",
    "            model.add(BatchNormalization())\n",
    "            model.layers[-1].set_weights([gamma, beta, mean, var]) #set weights in batchNormalization\n",
    "            model.add(Activation('relu'))\n",
    "            \n",
    "        if 'pool_size' in x:\n",
    "            #add 3 pooling layers\n",
    "            model.add(MaxPooling1D(pool_size=x['pool_size'],\n",
    "                                   strides=x['pool_strides'],\n",
    "                                   padding='valid'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(audio):\n",
    "    audio *= 256.0  # SoundNet needs the range to be between -256 and 256\n",
    "    # reshaping the audio data so it fits into the graph (batch_size, num_samples, num_filter_channels)\n",
    "    audio = np.reshape(audio, (1, -1, 1))\n",
    "    return audio\n",
    "\n",
    "\n",
    "def load_audio(audio_file):\n",
    "    sample_rate = 22050  # SoundNet works on mono audio files with a sample rate of 22050.\n",
    "    audio, sr = librosa.load(audio_file, dtype='float32', sr=22050, mono=True) #load audio\n",
    "    audio = preprocess(audio) #preprocess using soundnet parameters\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding1d_1 (ZeroPaddin (1, None, 1)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (1, None, 16)             1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (1, None, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (1, None, 16)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (1, None, 16)             0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_2 (ZeroPaddin (1, None, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (1, None, 32)             16416     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (1, None, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (1, None, 32)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (1, None, 32)             0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_3 (ZeroPaddin (1, None, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (1, None, 64)             32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (1, None, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (1, None, 64)             0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_4 (ZeroPaddin (1, None, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (1, None, 128)            65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (1, None, 128)            512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (1, None, 128)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_5 (ZeroPaddin (1, None, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (1, None, 256)            131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (1, None, 256)            1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_6 (ZeroPaddin (1, None, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (1, None, 512)            524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (1, None, 512)            2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (1, None, 512)            0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_7 (ZeroPaddin (1, None, 512)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (1, None, 1024)           2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (1, None, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "zero_padding1d_8 (ZeroPaddin (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (1, None, 401)            3285393   \n",
      "=================================================================\n",
      "Total params: 6,163,777\n",
      "Trainable params: 6,159,713\n",
      "Non-trainable params: 4,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "#Review of the model and structure\n",
    "model = build_model()\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_scenes(prediction):\n",
    "    scenes = []\n",
    "    with open('../soundnet_keras-master/categories/categories_places2.txt', 'r') as f:\n",
    "        categories = f.read().split('\\n')\n",
    "        for p in range(prediction.shape[1]):\n",
    "            scenes.append(categories[np.argmax(prediction[0, p, :])])\n",
    "    return scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the prediction?\n",
    "# let's load a audio file\n",
    "audiot,sr = librosa.load('../soundnet_keras-master/railroad_audio.wav', dtype='float32', sr=22050, mono=True) #load audio\n",
    "\n",
    "#library to listen sound\n",
    "#import IPython.display as ipd\n",
    "\n",
    "#ipd.Audio(audiot) # load \n",
    "\n",
    "#sound like a railroad non?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 401)\n"
     ]
    }
   ],
   "source": [
    "def predict_scene_from_audio_file(audio_file):\n",
    "    model = build_model()\n",
    "    audio = load_audio(audio_file)\n",
    "    return model.predict(audio)\n",
    "\n",
    "prediction = predict_scene_from_audio_file('../soundnet_keras-master/railroad_audio.wav')\n",
    "print(prediction.shape)\n",
    "#import seaborn as sns\n",
    "#plt.figure(figsize=(8,4))\n",
    "#sns.countplot(x='label', data=prediction);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4e35328ee48c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#histograma con ventana\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(prediction[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experimentos con otros audios uno largo (demo o propio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descripcion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/homes/k18ibarr/Téléchargements/Projet/ESC-50-master/meta/esc50.csv',sep=',')\n",
    "test.head()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "list_target = []\n",
    "list_category = []\n",
    "\n",
    "for file_name, target, category,esc10 in zip(list(test['filename']), \n",
    "                                       list(test['target']), \n",
    "                                       list(test['category']),\n",
    "                                       list(test['esc10'])):\n",
    "        if esc10 == True: \n",
    "            #only 10 classes\n",
    "            audio = load_audio('/homes/k18ibarr/Téléchargements/Projet/ESC-50-master/audio/'+file_name)\n",
    "            data.append(audio)\n",
    "            list_target.append(target)\n",
    "            list_category.append(category)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = np.asarray([data[155]]).reshape(1,-1,1) #falla por tamano\n",
    "print(datos.shape)\n",
    "\n",
    "p = model.predict(datos) #-dgfsdfgsdfgdfg---menores a 5 segundos no funciona en el ultimo nivel\n",
    "print(p.shape)\n",
    "print(predictions_to_scenes(p))\n",
    "tensor = get.reshape(1,-1)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio 1-172649-b-40 helicopter numero 5 en data\n",
    "datos = np.asarray([data[5],data[5],data[5]]).reshape(1,-1,1)\n",
    "print(datos.shape)\n",
    "p = model.predict(datos) #-dgfsdfgsdfgdfg---menores a 5 segundos no funciona en el ultimo nivel\n",
    "print(p.shape)\n",
    "print(predictions_to_scenes(p))\n",
    "tensor = get.reshape(1,-1)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nuestros experimentos\n",
    "\n",
    "1. Encontrar caracteristicas en las hidden layers transfer  learning graficas tsne\n",
    "2. clasicador con la extraccion de pool5 menos parametros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conclusiones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-eac66ec603da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconclusiones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'conclusiones' is not defined"
     ]
    }
   ],
   "source": [
    "conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
